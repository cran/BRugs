<!DOCTYPE html PUBLIC "-//W3C//DTD Html 1.0 Strict//EN" "http://www.w3.org/TR/Html1/DTD/strict.dtd"><html><head><title>Tricks</title></head><body><p><font face="Arial" color="#000100" size="2"><img src="bugsicon.jpg" alt="[bugsicon]"></font><font face="Arial" color="#000100" size="6"><strong>&nbsp;&nbsp;&nbsp;Tricks: Advanced Use of the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BUGS Language</strong></font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#0000FF" size="2"><br></font><font face="Arial" color="#000100" size="2"><a id="Contents"></font><font face="Arial" color="#000100" size="5"><strong>Contents</strong></font><font face="Arial" color="#000100" size="2"></a><br><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#SpecifyingANewSamplingDistribution"></font><font face="Arial" color="#0000FF" size="4"><u>Specifying&nbsp;a&nbsp;new&nbsp;sampling&nbsp;distribution</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#SpecifyingANewPriorDistribution"></font><font face="Arial" color="#0000FF" size="4"><u>Specifying&nbsp;a&nbsp;new&nbsp;prior&nbsp;distribution</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#SpecifyingADiscretePriorOnASetOfValues"></font><font face="Arial" color="#0000FF" size="4"><u>Specifying&nbsp;a&nbsp;discrete&nbsp;prior&nbsp;on&nbsp;a&nbsp;set&nbsp;of&nbsp;values</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#UsingPDAndDIC"></font><font face="Arial" color="#0000FF" size="4"><u>Using&nbsp;pD&nbsp;and&nbsp;DIC</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#MixturesOfModelsOfDifferentComplexity"></font><font face="Arial" color="#0000FF" size="4"><u>Mixtures&nbsp;of&nbsp;models&nbsp;of&nbsp;different&nbsp;complexity</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#WhereTheSizeOfASetIsARandomQuantity"></font><font face="Arial" color="#0000FF" size="4"><u>Where&nbsp;the&nbsp;size&nbsp;of&nbsp;a&nbsp;set&nbsp;is&nbsp;a&nbsp;random&nbsp;quantity</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#AssessingSensitivityToPriorAssumptions"></font><font face="Arial" color="#0000FF" size="4"><u>Assessing&nbsp;sensitivity&nbsp;to&nbsp;prior&nbsp;assumptions</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#ModellingUnknownDenominators"></font><font face="Arial" color="#0000FF" size="4"><u>Modelling&nbsp;unknown&nbsp;denominators</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#HandlingUnbalancedDatasets"></font><font face="Arial" color="#0000FF" size="4"><u>Handling&nbsp;unbalanced&nbsp;datasets</u></font><font face="Arial" color="#000100" size="4"></a><br></font><font face="Arial" color="#000100" size="1">&nbsp;&nbsp;&nbsp;</font><font face="Arial" color="#000100" size="4"><a href="#UseOfTheCutFunction"></font><font face="Arial" color="#0000FF" size="4"><u>Use&nbsp;of&nbsp;the&nbsp;&quot;cut&quot;&nbsp;function</u></font><font face="Arial" color="#000100" size="4"></a></font><font face="Arial" color="#000100" size="2"><br><br><a id="SpecifyingANewSamplingDistribution"></font><font face="Arial" color="#000100" size="5"><strong>Specifying a new sampling distribution</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000100" size="3">Suppose we wish to use a sampling distribution that is not included in the </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/Distributions.html"></font><font face="Arial" color="#0000FF" size="3"><u>list&nbsp;of&nbsp;standard&nbsp;distributions</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000100" size="3">, in which an </font><font face="Arial" color="#000000" size="3">observation x[i] contributes a likelihood term L[i]</font><font face="Arial" color="#000100" size="3">. </font><font face="Arial" color="#000000" size="3">We may use the &quot;zeros trick&quot;: a Poisson(phi) observation of zero has likelihood exp(-phi), so if our observed data is a set of 0's, and</font><font face="Arial" color="#000000" size="3"><em> </em></font><font face="Arial" color="#000000" size="3">phi[i] is set to </font><font face="Arial" color="#000100" size="3">-</font><font face="Arial" color="#000000" size="3">log(L[i]), we will obtain the correct likelihood contribution. (Note that phi[i] should always be &gt; 0 as it is a Poisson mean, and so we may need to add a suitable constant to ensure that it is positive.) This trick is illustrated by an example </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/new-sampling.html"></font><font face="Arial" color="#0000FF" size="3"><u>new-sampling</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> in which a normal likelihood is constructed (using the zeros trick) and compared to the standard analysis.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;C &lt;- 10000</font><font face="Arial" color="#000000" size="3">&nbsp;&nbsp;&nbsp;# this just has to be large enough to ensure all phi[i]'s &gt; 0<br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;for (i in 1:N) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;zeros[i] &lt;- 0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;phi[i] &lt;- -log(L[i]) + C<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;zeros[i] ~ dpois(phi[i])<br>&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#000000" size="3"><br>This trick allows arbitrary sampling distributions to be used, and is particularly suitable when, say, dealing with truncated distributions.<br><br>A new observation x.pred can be predicted by specifying it as missing in the data-file and assigning it a uniform prior, e.g.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;x.pred ~ dflat()</font><font face="Arial" color="#000000" size="3">&nbsp;&nbsp;&nbsp;# improper uniform prior on new x</font><font face="Arial" color="#000100" size="3"><br></font><font face="Arial" color="#000000" size="3"><br>However our example shows that this method can be very inefficient and give a very high MC error.<br><br>An alternative to using 'zeros' is the &quot;ones trick&quot;, where the data is a set of 1's assumed to be the results of Bernoulli trials with probabilities p[i]. By making each p[i] proportional to L[i] (i.e. by specifying a scaling constant large enough to ensure all p[i]'s are &lt; 1) the required likelihood term is provided.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;C &lt;- 10000</font><font face="Arial" color="#000000" size="3">&nbsp;&nbsp;&nbsp;# this just has to be large enough to ensure all p[i]'s &lt; 1</font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;for (i in 1:N) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ones[i] &lt;- 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p[i] &lt;- L[i] / C<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ones[i] ~ dbern(p[i])<br>&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#0000FF" size="2"><br></font><font face="Arial" color="#000100" size="2"><a id="SpecifyingANewPriorDistribution"></font><font face="Arial" color="#000100" size="5"><strong>Specifying a new prior distribution</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">If, for a parameter </font><font face="Arial" color="#000000" size="3"><em>theta</em></font><font face="Arial" color="#000000" size="3">, say, we want to use a prior distribution that is not part of the </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/Distributions.html"></font><font face="Arial" color="#0000FF" size="3"><u>standard&nbsp;set</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3">, then we can use the 'zeros' trick (see above) at the prior level. A single zero Poisson observation (with mean phi = phi(</font><font face="Arial" color="#000000" size="3"><em>theta</em></font><font face="Arial" color="#000000" size="3">)) contributes a term exp(-phi) to the likelihood for </font><font face="Arial" color="#000000" size="3"><em>theta</em></font><font face="Arial" color="#000000" size="3">; when this is combined with a 'flat' prior for </font><font face="Arial" color="#000000" size="3"><em>theta</em></font><font face="Arial" color="#000000" size="3"> the correct prior distribution results.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;zero &lt;- 0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;theta ~ dflat()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;phi &lt;- </font><font face="Arial" color="#000000" size="3"><em>expression for </em></font><font face="Arial" color="#000100" size="3">-</font><font face="Arial" color="#000000" size="3">log(</font><font face="Arial" color="#000000" size="3"><em>desired prior for theta</em></font><font face="Arial" color="#000000" size="3">)</font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;zero ~ dpois(phi)<br></font><font face="Arial" color="#000000" size="3"><br>This is illustrated in </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/new-prior.html"></font><font face="Arial" color="#0000FF" size="3"><u>new-prior</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> by an example in which a normal prior is constructed using the zeros trick and the results are compared to the standard formulation. It is important to note that this method produces high auto-correlation, poor convergence and high MC error, so it is computationally slow and long runs are necessary.</font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000100" size="2"><a id="SpecifyingADiscretePriorOnASetOfValues"></font><font face="Arial" color="#000100" size="5"><strong>Specifying a discrete prior on a set of values</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000100" size="3">Suppose you want a parameter D to take one of a set of values, d[1], ..., d[K], say, with probabilities p[1], ..., p[K]. Then specify the arrays d[1:K] and p[1:K] in a data-file (or in the model) and use:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M ~ dcat(p[])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# choose which element of d[] to use.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D &lt;- d[M]<br><br>This is illustrated by an example </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/t-df.html"></font><font face="Arial" color="#0000FF" size="3"><u>t-df</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000100" size="3"> of learning about the degrees of freedom of a t-distribution.<br><br>If the discrete prior is put on too coarse a grid, then there may be numerical problems (crashes), unless good initial values are provided, and very poor mixing. It is therefore advised to either use a continuous prior or a discrete prior on a fine grid - see the </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/t-df.html"></font><font face="Arial" color="#0000FF" size="3"><u>t-df</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000100" size="3"> example.<br><br></font><font face="Arial" color="#000100" size="2"><a id="UsingPDAndDIC"></font><font face="Arial" color="#000100" size="5"><strong>Using pD and DIC</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Here we make a number of observations regarding the use of DIC and pD </font><font face="Arial" color="#000100" size="3">-</font><font face="Arial" color="#000000" size="3"> for a full discussion see </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/References.html#DICPaper"></font><font face="Arial" color="#0000FF" size="3"><u>Spiegelhalter&nbsp;</u></font><font face="Arial" color="#0000FF" size="3"><em><u>et&nbsp;al</u></em></font><font face="Arial" color="#0000FF" size="3"><u>.&nbsp;(2002)</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000100" size="3">:</font><font face="Arial" color="#000000" size="3"><br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000000" size="3">1) DIC is intended as a generalisation of Akaike's Information Criterion (AIC). For non-hierarchical models, pD should be approximately the true number of parameters.<br><br>2) Slightly different values of Dhat (and hence pD and DIC) can be obtained depending on the parameterisation used for the prior distribution. For example, consider the precision </font><font face="Arial" color="#000000" size="3"><em>tau</em></font><font face="Arial" color="#000000" size="3"> (1 / variance) of a normal distribution. The two priors<br></font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tau ~ dgamma(0.001, 0.001)</font><font face="Arial" color="#000000" size="3"> and<br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log.tau ~ dunif(-10, 10); log(tau) &lt;- log.tau</font><font face="Arial" color="#000000" size="3"><br></font><font face="Arial" color="#000100" size="3"><br></font><font face="Arial" color="#000000" size="3">are essentially identical but will give slightly different results for Dhat: for the first prior the stochastic parent is </font><font face="Arial" color="#000000" size="3"><em>tau</em></font><font face="Arial" color="#000000" size="3"> and hence the posterior mean of </font><font face="Arial" color="#000000" size="3"><em>tau</em></font><font face="Arial" color="#000000" size="3"> is substituted in Dhat, while in the second parameterisation the stochastic parent is </font><font face="Arial" color="#000000" size="3"><em>log.tau</em></font><font face="Arial" color="#000000" size="3"> and hence the posterior mean of log(</font><font face="Arial" color="#000000" size="3"><em>tau</em></font><font face="Arial" color="#000000" size="3">)<br>is substituted in Dhat.<br><br>3) For sampling distributions that are log-concave in their stochastic parents, pD is guaranteed to be positive (provided the simulation has converged). However, it is theoretically possible to get negative values. We have obtained negative pD's in the following situations:</font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000000" size="3">i) with non-log-concave likelihoods (e.g. Student-t distributions) when there is substantial conflict between prior and data;<br>ii) when the posterior distribution for a parameter is symmetric and bimodal, and so the posterior mean is a very poor summary statistic and gives a very large deviance.</font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000000" size="3">4) No MC error is available on the DIC. MC error on Dbar can be obtained by monitoring deviance and is generally quite small. The primary concern is to ensure convergence of Dbar </font><font face="Arial" color="#000100" size="3">-</font><font face="Arial" color="#000000" size="3"> it is therefore worthwhile checking the stability of Dbar over a long chain.<br><br>5) The minimum DIC estimates the model that will make the best short-term predictions, in the same spirit as Akaike's criterion. However, if the difference in DIC is, say, less than 5, and the models make very different inferences, then it could be misleading just to report the model with the lowest DIC.<br><br>6) DICs are comparable only over models with exactly the same observed data, but there is no need for them to be nested.<br><br>7) DIC differs from Bayes factors and BIC in both form and aims.<br><br>8) Caution is advisable in the use of DIC until more experience has been gained. </font><font face="Arial" color="#000000" size="3"><strong>It is important to note that the calculation of DIC will be disallowed for certain models. Please see the WinBUGS 1.4 web-page for details:</strong></font><font face="Arial" color="#000000" size="2"><strong><br><br></strong></font><font face="Arial" color="#000000" size="3">http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/contents.shtml</font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000100" size="2"><a id="MixturesOfModelsOfDifferentComplexity"></font><font face="Arial" color="#000100" size="5"><strong>Mixtures of models of different complexity</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Suppose we assume that each observation, or group of observations, is from one of a set of distributions, where the members of the set have different complexity. For example, we may think data for each person's growth curve comes from either a linear or quadratic line. We might think we would require 'reversible jump' techniques, but this is not the case as we are really only considering a single mixture model as a sampling distribution. Thus standard methods for setting up mixture distributions can be adopted, but with components having different numbers of parameters.<br><br>The </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/mixtures.html"></font><font face="Arial" color="#0000FF" size="3"><u>mixtures</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> example illustrates how this is handled in WinBUGS, using a set of simulated data.<br><br>Naturally, the standard warnings about mixture distributions apply, in that convergence may be poor and careful parameterisation may be necessary to avoid some of the components becoming empty.<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000100" size="2"><a id="WhereTheSizeOfASetIsARandomQuantity"></font><font face="Arial" color="#000100" size="5"><strong>Where the size of a set is a random quantity</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Suppose the size of a set is a random quantity: this naturally occurs in 'changepoint' problems where observations up to an unknown changepoint K come from one model, and after K come from another. Note that we </font><font face="Arial" color="#000000" size="3"><strong>cannot</strong></font><font face="Arial" color="#000000" size="3"> use the construction<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i in 1:K) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i] ~ </font><font face="Arial" color="#000000" size="3"><em>model 1</em></font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i in (K + 1):N) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i] ~ </font><font face="Arial" color="#000000" size="3"><em>model 2</em></font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#0000FF" size="3"><br></font><font face="Arial" color="#000000" size="3">since the index for a loop cannot be a random quantity. Instead we can use the step function to set up an indicator as to which set each observation belongs to:<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i in 1:N) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ind[i] &lt;- 1 + step(i - K - 0.01)</font><font face="Arial" color="#000000" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# will be 1 for all i &lt;= K, 2 otherwise</font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i] ~ </font><font face="Arial" color="#000000" size="3"><em>model </em></font><font face="Arial" color="#000100" size="3">ind[i]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#000000" size="3"><br>This is illustrated in </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/random-sets.html"></font><font face="Arial" color="#0000FF" size="3"><u>random-sets</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> by the problem of adding up terms in a series of unknown length, and in </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/Stagnant.html"></font><font face="Arial" color="#0000FF" size="3"><u>Stagnant</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> by a changepoint problem.<br></font><font face="Arial" color="#0000FF" size="2"><br></font><font face="Arial" color="#000100" size="2"><a id="AssessingSensitivityToPriorAssumptions"></font><font face="Arial" color="#000100" size="5"><strong>Assessing sensitivity to prior assumptions</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">One way to do this is to repeat the analysis under different prior assumptions, but within the same simulation in order to aid direct comparison of results. Assuming the consequences of K prior distributions are to be compared:<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000000" size="3">a) replicate the dataset K times within the model code;<br>b) set up a loop to repeat the analysis for each prior, holding results in arrays;<br>c) compare results using the </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/The Inference Menu.html#Compare"></font><font face="Arial" color="#0000FF" size="3"><u>'compare'</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> facility.</font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000000" size="3">The example </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/prior-sensitivity.html"></font><font face="Arial" color="#0000FF" size="3"><u>prior-sensitivity</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> explores six different suggestions for priors on the random-effects variance in a meta-analysis.<br></font><font face="Arial" color="#000100" size="2"><br><a id="ModellingUnknownDenominators"></font><font face="Arial" color="#000100" size="5"><strong>Modelling unknown denominators</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Suppose we have an unknown Binomial denominator for which we wish to express a prior distribution. It can be given a Poisson prior but this makes it difficult to express a reasonably uniform distribution. Alternatively a continuous distribution could be specified and then the 'round' function used. For example, suppose we are told that a fair coin has come up heads 10 times </font><font face="Arial" color="#000100" size="3">-</font><font face="Arial" color="#000000" size="3"> how many times has it been tossed?<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r &lt;- 10<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p &lt;- 0.5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r ~ dbin(p, n)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n.cont ~ dunif(1, 100)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n &lt;- round(n.cont)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000100" size="3"><strong>&nbsp;&nbsp;&nbsp;node&nbsp;&nbsp;&nbsp;mean&nbsp;&nbsp;&nbsp;sd&nbsp;&nbsp;&nbsp;MC error&nbsp;&nbsp;&nbsp;2.5%&nbsp;&nbsp;&nbsp;median&nbsp;&nbsp;&nbsp;97.5%&nbsp;&nbsp;&nbsp;start&nbsp;&nbsp;&nbsp;sample<br></strong></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;n&nbsp;&nbsp;&nbsp;21.08&nbsp;&nbsp;&nbsp;4.794&nbsp;&nbsp;&nbsp;0.07906&nbsp;&nbsp;&nbsp;13.0&nbsp;&nbsp;&nbsp;21.0&nbsp;&nbsp;&nbsp;32.0&nbsp;&nbsp;&nbsp;1001&nbsp;&nbsp;&nbsp;5000<br>&nbsp;&nbsp;&nbsp;n.cont&nbsp;&nbsp;&nbsp;21.08&nbsp;&nbsp;&nbsp;4.804&nbsp;&nbsp;&nbsp;0.07932&nbsp;&nbsp;&nbsp;13.31&nbsp;&nbsp;&nbsp;20.6&nbsp;&nbsp;&nbsp;32.0&nbsp;&nbsp;&nbsp;1001&nbsp;&nbsp;&nbsp;5000<br>&nbsp;&nbsp;&nbsp;<br></font><font face="Arial" color="#000000" size="3">Assuming a uniform prior for the number of tosses, we can be 95% sure that the coin has been tossed between 13 and 32 times. </font><font face="Arial" color="#000100" size="3">A discrete prior on the integers could also have been used in this context.<br></font><font face="Arial" color="#000100" size="2"><br><a id="HandlingUnbalancedDatasets"></font><font face="Arial" color="#000100" size="5"><strong>Handling unbalanced datasets</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Suppose we observe the following data on three individuals:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Person 1: 13.2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Person 2: 12.3 , 14.1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Person 3: 11.0, 9.7, 10.3, 9.6<br>&nbsp;&nbsp;&nbsp;<br>There are three different ways of entering such 'ragged' data into WinBUGS:<br><br></font><font face="Arial" color="#000000" size="3"><strong>1. Fill-to-rectangular:</strong></font><font face="Arial" color="#000000" size="3"> Here the data is 'padded out' by explicitly including the missing data, i.e.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;y[,1]&nbsp;&nbsp;&nbsp;y[,2]&nbsp;&nbsp;&nbsp;y[,3]&nbsp;&nbsp;&nbsp;y[,4]<br>&nbsp;&nbsp;&nbsp;13.2&nbsp;&nbsp;&nbsp;NA&nbsp;&nbsp;&nbsp;NA&nbsp;&nbsp;&nbsp;NA<br>&nbsp;&nbsp;&nbsp;12.3&nbsp;&nbsp;&nbsp;14.1&nbsp;&nbsp;&nbsp;NA&nbsp;&nbsp;&nbsp;NA<br>&nbsp;&nbsp;&nbsp;11.0&nbsp;&nbsp;&nbsp;9.7&nbsp;&nbsp;&nbsp;10.3&nbsp;&nbsp;&nbsp;9.6<br>END<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000000" size="3">or list(y = structure(.Data = c(13.2, NA, NA, NA, 12.3, 14.1, NA, NA, 11.0, 9.7, 10.3, 9.6), .Dim = c(3, 4)).<br><br>A model such as </font><font face="Arial" color="#000100" size="3">y[i, j] ~ dnorm(mu[i], 1)</font><font face="Arial" color="#000000" size="3"> can then be fitted. This approach is inefficient unless one explicitly wishes to estimate the missing data.<br><br></font><font face="Arial" color="#000000" size="3"><strong>2. Nested indexing:</strong></font><font face="Arial" color="#000000" size="3"> Here the data are stored in a single array and the associated person is recorded as a factor, i.e.<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;y[]&nbsp;&nbsp;&nbsp;person[]<br>&nbsp;&nbsp;&nbsp;13.2&nbsp;&nbsp;&nbsp;1<br>&nbsp;&nbsp;&nbsp;12.3&nbsp;&nbsp;&nbsp;2<br>&nbsp;&nbsp;&nbsp;14.1&nbsp;&nbsp;&nbsp;2<br>&nbsp;&nbsp;&nbsp;11.0&nbsp;&nbsp;&nbsp;3<br>&nbsp;&nbsp;&nbsp;9.7&nbsp;&nbsp;&nbsp;3<br>&nbsp;&nbsp;&nbsp;10.3&nbsp;&nbsp;&nbsp;3<br>&nbsp;&nbsp;&nbsp;9.6&nbsp;&nbsp;&nbsp;3<br>END<br></font><font face="Arial" color="#000000" size="2"><br></font><font face="Arial" color="#000000" size="3">or list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), person = c(1, 2, 2, 3, 3, 3, 3)).<br><br>A model such as </font><font face="Arial" color="#000100" size="3">y[k] ~ dnorm(mu[person[k]], 1)</font><font face="Arial" color="#000000" size="3"> can then be fitted. This seems an efficient and clear way to handle the problem.<br><br></font><font face="Arial" color="#000000" size="3"><strong>3. Offset: </strong></font><font face="Arial" color="#000000" size="3">Here an 'offset' array holds the position in the data array at which each </font><font face="Arial" color="#000100" size="3">person's data starts. For example, the data might be</font><font face="Arial" color="#000000" size="3"><br><br>list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), offset = c(1, 2, 4, 8))<br><br>and lead to a model containing the code<br></font><font face="Arial" color="#000100" size="3"><br>&nbsp;&nbsp;&nbsp;for (k in offset[i]:(offset[i + 1] - 1)) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[k] ~ dnorm(mu[i], 1)<br>&nbsp;&nbsp;&nbsp;}<br><br>The danger with this method is that it relies on getting the offsets correct and they are difficult to check. </font><font face="Arial" color="#000000" size="3">See the </font><font face="Arial" color="#0000FF" size="3"><a href="../Docu/ragged.html"></font><font face="Arial" color="#0000FF" size="3"><u>ragged</u></font><font face="Arial" color="#0000FF" size="3"></a></font><font face="Arial" color="#000000" size="3"> example for a full worked example of these methods.<br></font><font face="Arial" color="#000000" size="2"><br><br></font><font face="Arial" color="#000100" size="2"><a id="UseOfTheCutFunction"></font><font face="Arial" color="#000100" size="5"><strong>Use of the &quot;cut&quot; function</strong></font><font face="Arial" color="#000100" size="2"></a> </font><font face="Arial" color="#0000FF" size="3">[<a href="#Contents">top</a>]</font><font face="Arial" color="#000100" size="2"><br></font><font face="Arial" color="#000000" size="3">Suppose we observe some data that we do not wish to contribute to the parameter estimation and yet we wish to consider as part of the model. This might happen, for example:<br><br>a) when we wish to make predictions on some individuals on whom we have observed some partial data that we do not wish to use for parameter estimation;<br>b) when we want to use data to learn about some parameters and not others;<br>c) when we want evidence from one part of a model to form a prior distribution for a second part of the model, but we do not want 'feedback' from this second part.<br><br>The &quot;cut&quot; function forms a kind of 'valve' in the graph: prior information is allowed to flow 'downwards' through the cut, but likelihood information is prevented from flowing upwards.<br><br>For example, the following code leaves the distribution for theta unchanged by the observation y.<br><br></font><font face="Arial" color="#000100" size="3">&nbsp;&nbsp;&nbsp;model <br>&nbsp;&nbsp;&nbsp;{<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y &lt;- 2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y ~ dnorm(theta.cut, 1)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;theta.cut &lt;- cut(theta)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;theta ~ dnorm(0, 1)<br>&nbsp;&nbsp;&nbsp;}<br></font><font face="Arial" color="#000000" size="2"><br></font></p></body></html>